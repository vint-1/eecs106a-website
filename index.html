<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="Object-centric environment for comparison between classical and unsupervised approaches to learning problems">
  <meta name="keywords" content="EECS106A">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>EECS 106A Final Project: Object-centric Environment for Comparison between Classical and Unsupervised Approaches to Learning Problems</title>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/favicon.svg">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>
<body>

<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">EECS 106A Final Project</h1>
          <h1 class="title is-4 publication-subtitle">Object-centric Environment for Comparison between Classical and Unsupervised Approaches to Learning Problems</h1>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="https://github.com">Yarden Goraly</a>,</span>
            <span class="author-block">
              <a href="https://github.com">Nathan Juan</a>,</span>
            <span class="author-block">
              <a href="https://github.com">Rishi Yang</a>,</span>
            <span class="author-block">
              <a href="https://github.com">Vint Lee</a>
            </span>
          </div>

          <!-- <div class="is-size-5 publication-authors">
            <span class="author-block">UC Berkeley</span>
          </div> -->

          <div class="column has-text-centered">
            <!-- TODO: update the links -->
            <div class="publication-links">
              <!-- Code Link. -->
              <span class="link-block">
                <a href="https://github.com/nathanjuan/eecs106a-final-project"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                  </a>
              </span>
            </div>

          </div>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- Teaser image (insert video demo here maybe?)
<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
        <img src="./static/images/teaser.png" class="teaser-image">
      <h2 class="subtitle has-text-centered">
        <span class="technique-name">Yes</span> insert caption here
      </h2>
    </div>
  </div>
</section> -->


<section class="hero is-light is-small">
  <div class="hero-body">
    <div class="container">
      <!-- Abstract. -->
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3">Abstract</h2>
          <div class="content has-text-justified">
            <p>
              We designed and implemented a robot that sorts blocks according to color into goal locations that it must 
              discover through trial and error. We designed an environment with two goal locations and blocks of two colors, 
              red and green, where red or green have an optimal goal location which does not depend of the other color. 
              This project was implemented using 'classical' robotic methods using HSV color sorting, an inverse kinematics
              controller, and preprogrammed assumtions about the table-block environment. This project is designed to 
              serve as a baseline comparison to unsupervised learning based implementations of object perception and 
              robotic manupulation. We also investigated the use of a object-detection model trained with unsupervised learning
              to improve the perception system.
            </p>
          </div>
        </div>
      </div>
      <!--/ Abstract. -->
    </div>
  </div>
</section>


<section class="section">
  <div class="container is-max-desktop">
    
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Demo</h2>
        <div class="publication-video">
          <iframe width="700" height="450" src="https://drive.google.com/file/d/1u3dqee8H_GzkwsXws3VEJknnF1tTCwYQ/preview" frameborder="0" allowfullscreen></iframe>
        </div>
      </div>
    </div>

  </div>
</section>


<section class="section">
  <div class="container is-max-desktop">
    
    <!-- Introduction -->
    <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-3">Introduction</h2>
        <!-- TODO add figures for these -->
        <h3 class="title is-4">Motivation</h3>
        <div class="content has-text-justified">
          <p>
            This project was intended to contribute to research into unsupervised learning through robotic interaction, and more specifically 
            learning unsupervised object-centric representations. Object-centric representations have shown promising results in being able to provide powerful functionlity 
            models for physics understanding, multi-agent prediction, and planning and causal reasoning [<a href="https://arxiv.org/abs/2307.07147">Stocking</a>]. 
            Another motivation for us was to provide an experimental testing ground for future experiments in the Hybrid Systems Lab by setting up the UR5 arm and providing 
            intuitive documentation for future users.
            
            
          </p>
        </div>

        <!-- <h3 class="title is-4">Final Goals</h3>
        <div class="content has-text-justified">
          <p>
            Create an intuitive testing ground... (Combine to one section Goals?)
          </p>
        </div> -->

        <h3 class="title is-4">Original and Final Goals</h3>
        <div class="content has-text-justified">
          <p>
            We wanted to build a robotic system that could solve a task in an object-centric, partially-unobservable environment. 
            Our goal was to use a UR5 robot arm and an RGB/Depth camera to interact with with blocks
            and move them to a goal location. There would be two goal locations and the correct goal location will 
            be unknown to the robot at the beginning, and the robot will have to respond to outside feedback to learn the correct goal. 
            We aimed to develop an algorithm for solving the task using classical methods to serve as a baseline to compare ML approaches.
            Our reach goal was to replace the perception, action planning, or robotic control with an unsupervised deep learning algorithm. 
            In the end, we made progress an unsupervised object-centric deep neural network to replace the perception system.

            This kind of task is
            difficult for current machine learning techniques, but representations that can keep track of multiple objects
            and update knowledge about their respective properties should make it possible for machine learning to solve
            the task.
          </p>
        </div>

      </div>
    </div>
    <!--/ Introduction. -->

    <!-- Design -->
    <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-3">Design</h2>
       
        <!-- Overview. -->
        <h3 class="title is-4">Overview</h3>
        
        <div class="columns is-centered">

          <!-- Design description -->
          <div class="column">
            <div class="content has-text-justified">
              <p>
                Our design involved a the table-block environment which is observed by the camera. The camera reports 
                the state of the environment to an algorithm which directs the arm to change the environment. When blocks are placed onto a goal location, 
                the robot waits for human feedback to observe the reward state, which then inform the following actions of moveing blocks.
              </p>
            </div>
          </div>

          <!-- Diagram -->
          <div class="column">
            <div class="content">
              <!-- <h2 class="title is-5 plot-title">Architecture</h2> -->
              <img src="./static/images/software-design.png"
                 class="image"
                 width="50%"
                 alt="Software architecture."/>
            </div>
          </div>


        </div>
        <!--/ Overview. -->
        <!--/ Environment. -->
        <h3 class="title is-4">Environment</h3>
        <div class="content has-text-justified">
          <p>
            The environment consists of a table with rectangular wooden blocks that are red or green. The goal locations are blue cardboard circles taped to the table.
            It is assumed that each goal has a reward value associated with a color, for example, all red blocks will incur a reward of x if placed on goal y.
          </p>
        </div>
        <!--/ Environment. -->
        <!--/ Perception. -->
        <h3 class="title is-4">Perception</h3>
        <div class="content has-text-justified">
          <p>
            The perception system consists of a RGB-Depth camera mounted on a tripod that looks down onto the table. The blocks and goal locations are localized
            with respect to the camera usuing the camera intrinscts and the depth meansument. The camera is localized with respect to the UR5 are using an AR tag
            which is fixed to the base of the robot. Thus we are able to localize the blocks and goals with respect to the UR5 arm. The camera recognizes the 
            blocks using HSV color filtering and making masks for the colors red, green or blue. Just a color mask cannot deal with multiple blocks, so we apply 
            K-means clustering to the mask image to determine the number and centroids of blocks of the same color.

          </p>
        </div>
        <!--/ Perception. -->
        <!--/ UR5 Control. -->
        <h3 class="title is-4">UR5 Control and Human Feedback</h3>
        <div class="content has-text-justified">
          <p>
            We used the MoveIt inverse kinematic controller to control the UR5 arm. We wrote procudures to reliably pick and place blocks given a block's location 
            and a target location. Our sorting algorithm will first put a red block on a goal location. It will then wait for a human to input 
            into the terminal the current reward. It will continue testing all colors on all goals, and then put the blocks on the optimal goal to 
            maximize the reward.

          </p>
        </div>
        <!--/ UR5 Control. -->
      </div>
    </div>
    <!--/ Design. -->

    <!-- Implementation -->
    <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-3">Implementation</h2>
        
        <!-- Setup. -->
        <h3 class="title is-4">Components</h3>
          <div class="columns is-centered">
            <div class="column">
              <!-- Robodesk -->
              <div class="content">
                <h2 class="title is-5 plot-title">Robotic Arm</h2>
                <img src="./static/images/exp-robot.png"
                   class="image"
                   width="80%"
                   alt="robot arm"/>
              </div>
              <p>
                We used a UR5 Robot, which is a 6-DOF arm manufactured by Universal Robots. For control, we use the MoveIt package to plan and execute trajectories; this package has a backbone of IKFast, which is a library that generates analytical solutions to the inverse kinematics problem. The main purpose of this component is to move the gripper to positions where it can pick up and move objects as desired.
              </p>
            </div>
            <div class="column">
              <!-- Robodesk -->
              <div class="content">
                <h2 class="title is-5 plot-title">Gripper</h2>
                <img src="./static/images/exp-gripper.png"
                   class="image"
                   width="80%"
                   alt="robot arm"/>
              </div>
              <p>
                The end-effector tool used for our project is the Robotiq 2F-140 gripper, with 1 degree of freedom. This is primarily used for grabbing and releasing objects.
              </p>
            </div>
            <div class="column">
              <!-- Robodesk -->
              <div class="content">
                <h2 class="title is-5 plot-title">Camera</h2>
                <img src="./static/images/exp-camera.png"
                   class="image"
                   width="80%"
                   alt="robot arm"/>
              </div>
              <p>
                We used an Intel Realsense D435i camera, which provides both visual and depth information. This is mounted on a camera stand and is positioned in a top-down view of the working area. The main purposes of the camera are to localize the UR5 arm with respect to the rest of the environment and perceive objects of interest within the environment.
              </p>
            </div>
          </div>
        </div>
          <!-- <p>
            More text here if needed...
          </p> -->

        </div>
        <h3 class="title is-4">Experimental Setup</h3>
        <div class="content has-text-justified">
          
          

          <div class="column is-full-width"> 
            <img src="./static/images/exp-setup.png"
                   class="image"
                   width="35%"
                   alt="Experimental Setup."/>
          </div>

          <p>
            Here is the setup of the environment with all of the components together. The base of the UR5 robot is kept at a fixed position on the table, and with the camera on the stand, the AR tag on the wooden base frame of the robot is used to establish the position of the robot and camera relative to each other. Then, the various goal positions and blocks in the environment are processed by the camera and these objects are then accurately localized with respect to the rest of the system. Below shows the processed information of the enviroment within RViz.
          </p>

          <div class="column is-full-width"> 
            <img src="./static/images/rviz.png"
                   class="image"
                   width="70%"
                   alt="RViz representation of environment."/>
          </div>

          </div>
        <!--/ Experimental Setup -->

        <!--(OLD) Software. -->
        <!-- <h3 class="title is-4">Software</h3>
        
        <div class="columns is-centered"> -->

          <!-- Software description -->
          <!-- <div class="column">
            <div class="content has-text-justified">
              <p>
                Our Software uses X apporach and y design. The figure to the right shows the design...
              </p>
            </div>
          </div> -->

          <!-- Diagram -->
          <!-- <div class="column">
            <div class="content">
              <h2 class="title is-5 plot-title">Software Architecture</h2>
              <img src="./static/images/software-design.png"
                 class="image"
                 width="50%"
                 alt="Software architecture."/>
            </div>
          </div> -->


        <!-- </div> -->
        <!--/ (OLD) Software. -->

        <!--/ Software. -->
        <h3 class="title is-4">Software</h3>
        <div class="content has-text-justified">
          <p>
            We used ROS Noetic running on Ubuntu 20.04 LTS (Focal Fossa). We used the <a href="https://github.com/ros-industrial/universal_robot">ROS-Industrial Universal Robots</a> 
            repository on Github for MoveIt functionality on the UR5. We used the <a href="https://github.com/TAMS-Group/robotiq">Robotiq Fork by TAMS</a> to control the gripper in 
            ROS Noetic. We also used the <a href="https://github.com/ros-perception/ar_track_alvar">ar_track_alvar</a> package for AR tag functionality. 

            We structured our software into a perception node, and an executive node. The perception node published the location of the blocks and goals, while the 
            executive node directed the robot using the envirionment state and human input.
          </p>
        </div>
        <!--/ Software. --> 

        <!-- Unsupervised. -->
        <h3 class="title is-4">Unsupervised Approach</h3>
        
        <!-- Software description -->
        <div class="content has-text-justified">
          <p>
            We also performed some experiments on applying unsupervised learning to 
            improve the perception abilities of the system.
            Instead of relying on HSV thresholding, which has to be recalibrated and adjusted
            for new objects and lighting conditions, we instead tried using an object detection algorithm
            trained on a dataset consisting of blocks moving around a scene.
          </p>
          <p>
            Specifically, we used the model in <a href=https://arxiv.org/abs/2307.07147>Stocking et. al. 2023</a>,
            which is an unsupervised object detection algorithm based on
            <a href="https://proceedings.neurips.cc/paper/2021/hash/a860a7886d7c7e2a8d3eaac96f76dc0d-Abstract.html">SIMONe</a>
            . These algorithms produce object segmentation masks without requiring explicit object labels in the training dataset,
            which make them particularly useful for our application. The object masks can be easily be used to
            compute the 3D position of the object.
          </p>
        </div>

        <div class="column is-full-width">
          <div class="content">
            <img src="./static/images/unsupervised-architecture.png"
                class="image"
                width="80%"
                alt="Unsupervised Architecture."/>
          </div>
        </div>
        <!--/ Unsupervised. -->

      </div>
    <!-- </div> -->
    <!--/ Method Overview. -->
  <div class="container is-max-desktop">
    <!-- Results. -->
    <div class="columns is-centered">
      
        
      <div class="column is-full-width">
        <h2 class="title is-3">Results</h2>

        <h2 class="title is-4">Demonstrations</h2>
        <p>
          Below are some demos of the classical algorithm at work. We test each color in both goal positions, receiving feedback from humans to determine the optimal locations of all blocks. 
          Notice that the robot is able to accurately locate all positions and move accordingly!</p>
          <br>
        <iframe width="700" height="450" src="https://drive.google.com/file/d/1OyvE3OIxD8S3YmURUeYx-vx-J7NDZThn/preview" frameborder="0" allowfullscreen></iframe>
        <iframe width="700" height="450" src="https://drive.google.com/file/d/1-zwcGBHxTS_oxWrfbyMYRja4g0B7CQBW/preview" frameborder="0" allowfullscreen></iframe>
        <iframe width="700" height="450" src="https://drive.google.com/file/d/1z3NMPCjo6tBEj7hDgBF__E46KCKHqNFZ/preview" frameborder="0" allowfullscreen></iframe>
        <iframe width="700" height="450" src="https://drive.google.com/file/d/1u3dqee8H_GzkwsXws3VEJknnF1tTCwYQ/preview" frameborder="0" allowfullscreen></iframe>

      </div>
    </div>        
        
    <div class="columns is-centered">
      <div class="column">
        <!-- Something good -->
        <h3 class="title is-4">Unsupervised Learning</h3>
        <div class="content has-text-justified"> 
          <p>
            Training our model on the <a href="https://rohitgirdhar.github.io/CATER/">CATER dataset</a>,
            we find that our model shows modest performance, with good reconstruction (right, middle) of the video frames,
            but relatively poor object segmentation (right, bottom). Despite being able to make out some shapes, it
            has yet to generalize well to out-of-distribution objects.
          </p>
          <p>
            We believe that performance could be significantly improved with a more diverse
            and high-quality dataset. In particular, one with fewer objects that are the same shape as the blocks, so it more closely
            resembles our workspace. Additionally, a dataset with more object movement within each video could allow the model to better learn
            the nature of the objects, potentially allowing to better generalizability. Moreover, we believe further hyperparameter tuning could also improve the model.
          </p>
          <p>
            Our results show promise in being able to segment our environment into representation masks that correspond to different objects. 
            Given our time constraints, we were unable to get strong reconstructions on out-of-distribution data. If we had more time with the project,
            or if we decide to continue this line of research, we would first improve segmentation performance. Then, we would use the object masks to output a (x,y) coordinate
            of the object on the image place. Using camera intrinsics, we could get the 3D coordinate of the block, and replace the classical HSV thresholding technique. 
            Further research in this area would be to observe how disentangled latent variables can inform the robot on the properties of the object in order to improve performance of the task. 
          </p>
        </div>
        <!-- 2 -->
        <div class="content">
          <h2 class="title is-5 plot-title">Training Loss</h2>
              <p>
                This is an example training loss from one of our experiments. Training a single experiment took roughly half a day, so if we continue this project, 
                one goal would be to see a comparison of losses with varied hyperparameters. 
              </p>
              <p>

              </p>
          <img src="./static/images/training_loss_example.png"
              class="image"/>
        </div>
      </div>
      
      <div class="column">
        <!-- 1 -->
        <div class="content">
          <h2 class="title is-5 plot-title">Ground Truth, Reconstruction, and Segmentation Maps</h2>
          <img src="./static/images/unsupervised_result_example.gif"
              class="image"/>
        </div>
            
        
      </div>
    </div>
  </div>

  </div>
</section>


<footer class="footer">
  <div class="container">
    <div class="content has-text-centered">
      <a class="icon-link" href="https://github.com/" class="external-link" disabled>
        <i class="fab fa-github"></i>
      </a>
    </div>
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            Website template from <a
              href="https://github.com/nerfies/nerfies.github.io">Nerfies</a>
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

</body>
</html>
